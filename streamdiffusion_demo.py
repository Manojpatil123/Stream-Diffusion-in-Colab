# -*- coding: utf-8 -*-
"""StreamDiffusion Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ffuIbPu89yrnjTdNMEUtHk2b1aYd9nrt
"""

!pip install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121

!git clone https://github.com/cumulo-autumn/StreamDiffusion.git

# Commented out IPython magic to ensure Python compatibility.
# %cd StreamDiffusion

!python setup.py develop easy_install streamdiffusion[tensorrt]

!python -m streamdiffusion.tools.install-tensorrt

# Commented out IPython magic to ensure Python compatibility.
# %cd StreamDiffusion

"""## Text to Image Generation"""

from utils.wrapper import StreamDiffusionWrapper

stream = StreamDiffusionWrapper(
    model_id_or_path = "KBlueLeaf/kohaku-v2.1",
    lora_dict = None,
    t_index_list = [0, 16, 32, 45],
    frame_buffer_size = 1,
    width = 512,
    height = 512,
    warmup = 10,
    acceleration = "xformers",
    mode = "txt2img",
    use_denoising_batch = False,
    cfg_type = "none",
    seed = 2
)

prompt = "A girl with brown hair, smiling, and wearing a shirt"

stream.prepare(
    prompt = prompt,
    num_inference_steps = 50
)

output_image = stream()
output_image.save("images/outputs/output.png")

"""## Multi Text-to-image"""

stream = StreamDiffusionWrapper(
    model_id_or_path = "KBlueLeaf/kohaku-v2.1",
    lora_dict = None,
    t_index_list = [0, 16, 32, 45],
    frame_buffer_size = 3,
    width = 512,
    height = 512,
    warmup = 10,
    acceleration = "xformers",
    mode = "txt2img",
    use_denoising_batch = False,
    cfg_type = "none",
    seed = 2
)

new_prompt = "A girl with brown hair, smiling, and wearing a shirt"

stream.prepare(
    prompt = prompt,
    num_inference_steps = 50
)

output_images = stream()
for i, output_image in enumerate(output_images):
    output_image.save(f"images/outputs/output_{i}.png")

"""## Text to image without Wrapper"""

import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("KBlueLeaf/kohaku-v2.1").to(
    device = torch.device("cuda"),
    dtype=torch.float16
)

from diffusers import AutoencoderTiny
from streamdiffusion import StreamDiffusion
from streamdiffusion.image_utils import postprocess_image

stream = StreamDiffusion(
    pipe,
    t_index_list = [0, 16, 32, 45],
    torch_dtype=torch.float16,
    cfg_type = "none"
)

stream.load_lcm_lora()
stream.fuse_lora()

stream.vae = AutoencoderTiny.from_pretrained("madebyollin/taesd").to(
    device = pipe.device,
    dtype=pipe.dtype
)

pipe.enable_xformers_memory_efficient_attention()

stream.prepare(prompt)

for _ in range(4):
  stream()

x_output = stream.txt2img()

output_image = postprocess_image(x_output, output_type="pil")[0]

output_image.save("images/outputs/output.png")

"""## Img to Img Generation"""

stream = StreamDiffusionWrapper(
    model_id_or_path = "KBlueLeaf/kohaku-v2.1",
    lora_dict = None,
    t_index_list = [22, 32, 45],
    frame_buffer_size = 1,
    width = 512,
    height = 512,
    warmup = 10,
    acceleration = "xformers",
    mode = "img2img",
    use_denoising_batch = True,
    cfg_type = "self",
    seed = 2
)

prompt = "A girl with brown hair, smiling, and wearing a shirt"
negative_prompt = "low quality, blurry, low resolution"
image = "images/inputs/girl_img.png"

stream.prepare(
    prompt = prompt,
    negative_prompt = negative_prompt,
    num_inference_steps = 50,
    guidance_scale = 1.2,
    delta = 0.5
)

image_tensor = stream.preprocess_image(image)

for _ in range(stream.batch_size - 1):
  stream(image=image)

output_image = stream(image = image_tensor)

output_image.save("images/outputs/output5.png")

